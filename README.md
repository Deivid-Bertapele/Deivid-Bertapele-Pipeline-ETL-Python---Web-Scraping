# ğŸš€ Pipeline ETL - Mercado Livre Notebooks Analysis

![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)

Um pipeline ETL completo para anÃ¡lise de notebooks no Mercado Livre, incluindo coleta de dados, transformaÃ§Ã£o e visualizaÃ§Ã£o.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Funcionalidades](#-funcionalidades)
- [Tecnologias](#-tecnologias)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [Uso](#-uso)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Notebook de AnÃ¡lise](#-notebook-de-anÃ¡lise)
- [ContribuiÃ§Ã£o](#-contribuiÃ§Ã£o)
- [LicenÃ§a](#-licenÃ§a)

## ğŸŒŸ VisÃ£o Geral

Este projeto implementa um pipeline ETL (Extract, Transform, Load) para coletar, processar e analisar dados de notebooks disponÃ­veis no Mercado Livre. O pipeline inclui:

- Coleta de dados usando Scrapy
- TransformaÃ§Ã£o e limpeza dos dados
- Armazenamento em banco de dados SQLite
- VisualizaÃ§Ã£o interativa com Streamlit
- AnÃ¡lise detalhada em Jupyter Notebook

## âœ¨ Funcionalidades

- **Coleta de Dados**: ExtraÃ§Ã£o automÃ¡tica de informaÃ§Ãµes de notebooks
- **Dashboard Interativo**: VisualizaÃ§Ã£o de KPIs e anÃ¡lises
- **AnÃ¡lise de PreÃ§os**: ComparaÃ§Ã£o de preÃ§os por marca
- **AvaliaÃ§Ã£o de SatisfaÃ§Ã£o**: AnÃ¡lise de reviews e ratings
- **Filtros Inteligentes**: Filtragem de produtos por faixa de preÃ§o
- **AnÃ¡lise ExploratÃ³ria**: Notebook detalhado com insights e visualizaÃ§Ãµes

## ğŸ› ï¸ Tecnologias

- **Python 3.8+**
- **Pandas**: Processamento e anÃ¡lise de dados
- **Scrapy**: Web scraping
- **Streamlit**: Interface de visualizaÃ§Ã£o
- **SQLite**: Armazenamento de dados
- **Jupyter Notebook**: AnÃ¡lise exploratÃ³ria de dados
- **Matplotlib/Seaborn**: VisualizaÃ§Ã£o de dados

## ğŸ“¥ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone [URL_DO_REPOSITÃ“RIO]
cd PIPELINE_ETL_PYTHON_SCRAPY
```

2. Crie e ative um ambiente virtual:
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows
```

3. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

## ğŸš€ Uso

1. Execute o spider para coletar dados:
```bash
cd src/notebooks
scrapy crawl notebooks -O data/data.jsonl
```

2. Execute o pipeline ETL:
```bash
python src/transformLoad/main.py
```

3. Inicie o dashboard:
```bash
streamlit run src/dashboards/app.py
```

4. Abra o notebook de anÃ¡lise:
```bash
jupyter notebook src/notebooks/notebook_analysis.ipynb
```

## ğŸ“ Estrutura do Projeto

```
PIPELINE_ETL_PYTHON_SCRAPY/
â”œâ”€â”€ data/                  # Dados coletados e processados
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dashboards/       # AplicaÃ§Ã£o Streamlit
â”‚   â”œâ”€â”€ notebooks/        # Notebooks de anÃ¡lise e spiders
â”‚   â””â”€â”€ transformLoad/    # Scripts de transformaÃ§Ã£o
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md            # DocumentaÃ§Ã£o
```

--

# Deivid-Bertapele-Pipeline-ETL-Python---Web-Scraping
